{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install torch diffusers open-clip-torch Pillow numpy langchain langchain-groq gradio\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport open_clip\nfrom diffusers import StableDiffusionPipeline\nimport numpy as np\nfrom PIL import Image\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_groq import ChatGroq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clip_model, clip_tokenizer, preprocess = open_clip.create_model_and_transforms(\n    'ViT-B-32', pretrained='laion2b_s34b_b79k', device=device\n)\ntokenizer = clip_tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def blend_prompts_and_generate(prompt_a, prompt_b):\n    aesthetic_prompts=(f\"{prompt_a},{prompt_b},aesthetic,visually pleasing\")\n\n    model= \"runwayml/stable-diffusion-v1-5\"\n    pipe = StableDiffusionPipeline.from_pretrained(model,torch_dtype=torch.float16)\n    pipe = pipe.to(device)\n\n    llm = ChatGroq(\n    temperature=0.3,\n    model=\"llama3-70b-8192\",\n    api_key=\"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\"\n    )\n\n    system=(\n        \"Enhance the two prompts given by the user by adding descriptors\"\n        \"Enhanced prompts should have all objects and subjects provided by user\"\n        \"Enhance by adding relevant adjectives to the objects and subjects given in the prompt so as to create a visually pleasing image \"\n        \"Apply token weighting as follows: \"\n        \"- Main subject: (word:1.6-1.7) \"\n        \"- Important object: (word:1.2-1.3)\"\n        \"Main subject is what is provided in the prompts user\"\n        \"Do not distract from the prompts provided by user, what user has provided is the main subject\"\n        \"Limit your response to 10-12 words\"\n        \"Return only the two enhanced prompts with weights\"\n    )\n    prompt_template = ChatPromptTemplate.from_messages([\n        (\"system\", system),\n        (\"human\", \"{input}\")\n    ])\n    output_parser = StrOutputParser()  #This tells LangChain that the LLM's output should be treated as a plain string\n    chain = prompt_template | llm_1 | output_parser\n\n   def enhance_prompt(text):\n        return chain.invoke({\"input\": text})\n\n    enhanced_prompt_a = enhance_prompt(prompt_a)\n    enhanced_prompt_b = enhance_prompt(prompt_b)\n\n    text_input_a = pipe.tokenizer(\n        enhanced_prompt_a, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, return_tensors=\"pt\"\n    ).input_ids.to(device)\n    text_input_b = pipe.tokenizer(\n        enhanced_prompt_b, padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, return_tensors=\"pt\"\n    ).input_ids.to(device)\n\n    with torch.no_grad():\n        embeddings_a = pipe.text_encoder(text_input_a)[0]\n        embeddings_b = pipe.text_encoder(text_input_b)[0]\n\n    height, width = 512, 512\n    generator = torch.Generator(device=device).manual_seed(42)\n    latents = torch.randn(\n        (1, pipe.unet.in_channels, height // 8, width // 8),\n        generator=generator,\n        device=device,\n        dtype=torch.float16\n    )\n     def generate_blended_image(alpha):\n        blended_embeddings = alpha * embeddings_a + (1 - alpha) * embeddings_b\n        with torch.autocast(device):\n            image = pipe(\n                prompt_embeds=blended_embeddings,\n                latents=latents,\n                num_inference_steps=30,\n                guidance_scale=7.5,\n                height=height,\n                width=width\n            ).images[0]\n        return image\n\n    def calculate_aesthetic_score(image):\n        img_tensor = preprocess(image).unsqueeze(0).to(device)\n        text_tokens = tokenizer(aesthetic_prompts).to(device)\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            image_features = clip_model.encode_image(img_tensor)\n            text_features = clip_model.encode_text(text_tokens)\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n            text_features /= text_features.norm(dim=-1, keepdim=True)\n            similarity = (image_features @ text_features.T).softmax(dim=-1)\n            return similarity[0, 0].item()\n\n    alphas = np.linspace(0.4, 0.6, 20)\n    images, scores = [], []\n\n    for alpha in alphas:\n        image = generate_blended_image(alpha)\n        images.append(image)\n        scores.append(calculate_aesthetic_score(image))\n\n    best_idx = np.argmax(scores)\n    best_image = images[best_idx]\n    return best_image  # Return PIL image directly\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\n\ndef gradio_blend(prompt_a, prompt_b):\n    image = blend_prompts_and_generate(prompt_a, prompt_b)\n    return image\n\nwith gr.Blocks(theme=gr.themes.Soft(primary_hue=\"purple\")) as demo:\n    gr.Markdown(\n        \"\"\"\n        <h1 style='text-align: center; color: #6a0dad;'>✨ Prompt Blending ✨</h1>\n        <p style='text-align: center;'>Enter two prompts and generate a beautifully blended image.</p>\n        \"\"\"\n    )\n\n    with gr.Row():\n        with gr.Column():\n            prompt_a = gr.Textbox(label=\"Prompt A\", placeholder=\"Enter your first prompt...\")\n            prompt_b = gr.Textbox(label=\"Prompt B\", placeholder=\"Enter your second prompt...\")\n            generate_btn = gr.Button(\"Blend Prompts ✨\")\n        with gr.Column():\n            output_image = gr.Image(type=\"pil\", label=\"Blended Image\")\n\n    generate_btn.click(fn=gradio_blend, inputs=[prompt_a, prompt_b], outputs=output_image)\n\ndemo.launch(share=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}